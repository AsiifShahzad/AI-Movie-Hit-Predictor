{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43d5324c",
   "metadata": {},
   "source": [
    "# Movie Hit Prediction System\n",
    "\n",
    "## Project Overview\n",
    "This is a production-ready machine learning system designed to predict movie success through a two-stage approach:\n",
    "1. **Regression Stage**: Predict movie revenue before release\n",
    "2. **Classification Stage**: Categorize movies as Flop / Average / Hit\n",
    "\n",
    "## Phase 1: Data Loading and Consolidation\n",
    "\n",
    "This phase covers:\n",
    "- Loading both TMDB datasets\n",
    "- Schema inspection and comparison\n",
    "- Duplicate detection and removal\n",
    "- Data standardization\n",
    "- Consolidated dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00146285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Dataset 1 (02_raw_tmdb_5000_movies.csv) loaded: 4803 rows × 20 columns\n",
      "✓ Dataset 2 (01_raw_tmdb_10000_movies.csv) loaded: 10000 rows × 9 columns\n",
      "\n",
      "Total rows before consolidation: 14803\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(\"/home/asif/AI-Movie-Hit-Predictor\")\n",
    "DATA_DIR = PROJECT_ROOT / \"project_components\" / \"data\"\n",
    "DATASET_PATH_5K = DATA_DIR / \"02_raw_tmdb_5000_movies.csv\"\n",
    "DATASET_PATH_10K = DATA_DIR / \"01_raw_tmdb_10000_movies.csv\"\n",
    "\n",
    "df_5k = pd.read_csv(DATASET_PATH_5K)\n",
    "df_10k = pd.read_csv(DATASET_PATH_10K)\n",
    "\n",
    "print(f\"\\n✓ Dataset 1 (02_raw_tmdb_5000_movies.csv) loaded: {df_5k.shape[0]} rows × {df_5k.shape[1]} columns\")\n",
    "print(f\"✓ Dataset 2 (01_raw_tmdb_10000_movies.csv) loaded: {df_10k.shape[0]} rows × {df_10k.shape[1]} columns\")\n",
    "print(f\"\\nTotal rows before consolidation: {df_5k.shape[0] + df_10k.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250e9b4e",
   "metadata": {},
   "source": [
    "### Step 1.1: Dataset Comparison\n",
    "\n",
    "**Dataset 1 (5K)**: TMDB's well-curated dataset with comprehensive features\n",
    "- Budget and Revenue data\n",
    "- Production companies and keywords\n",
    "- Detailed metadata\n",
    "\n",
    "**Dataset 2 (10K)**: Extended TMDB dataset with more movies\n",
    "- May have partial overlap with 5K dataset\n",
    "- Smaller feature set (missing budget/revenue)\n",
    "- Useful for expanding coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d9520ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset 1 (5K) Columns\n",
      "Columns: ['budget', 'genres', 'homepage', 'id', 'keywords', 'original_language', 'original_title', 'overview', 'popularity', 'production_companies', 'production_countries', 'release_date', 'revenue', 'runtime', 'spoken_languages', 'status', 'tagline', 'title', 'vote_average', 'vote_count']\n",
      "Data Types:\n",
      "budget                    int64\n",
      "genres                      str\n",
      "homepage                    str\n",
      "id                        int64\n",
      "keywords                    str\n",
      "original_language           str\n",
      "original_title              str\n",
      "overview                    str\n",
      "popularity              float64\n",
      "production_companies        str\n",
      "production_countries        str\n",
      "release_date                str\n",
      "revenue                   int64\n",
      "runtime                 float64\n",
      "spoken_languages            str\n",
      "status                      str\n",
      "tagline                     str\n",
      "title                       str\n",
      "vote_average            float64\n",
      "vote_count                int64\n",
      "dtype: object\n",
      "\n",
      "--- Dataset 2 (10K) Columns ---\n",
      "Columns: ['id', 'original_language', 'original_title', 'overview', 'popularity', 'release_date', 'title', 'vote_average', 'vote_count']\n",
      "Data Types:\n",
      "id                     int64\n",
      "original_language        str\n",
      "original_title           str\n",
      "overview                 str\n",
      "popularity           float64\n",
      "release_date             str\n",
      "title                    str\n",
      "vote_average         float64\n",
      "vote_count             int64\n",
      "dtype: object\n",
      "\n",
      "Schema Analysis\n",
      "✓ Common columns (9): ['id', 'original_language', 'original_title', 'overview', 'popularity', 'release_date', 'title', 'vote_average', 'vote_count']\n",
      "✓ Unique to 5K dataset (11): ['budget', 'genres', 'homepage', 'keywords', 'production_companies', 'production_countries', 'revenue', 'runtime', 'spoken_languages', 'status', 'tagline']\n",
      "✓ Unique to 10K dataset (0): []\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDataset 1 (5K) Columns\")\n",
    "print(f\"Columns: {sorted(df_5k.columns.tolist())}\")\n",
    "print(f\"Data Types:\\n{df_5k.dtypes}\")\n",
    "\n",
    "print(\"\\n--- Dataset 2 (10K) Columns ---\")\n",
    "print(f\"Columns: {sorted(df_10k.columns.tolist())}\")\n",
    "print(f\"Data Types:\\n{df_10k.dtypes}\")\n",
    "\n",
    "# Find common and unique columns\n",
    "cols_5k = set(df_5k.columns)\n",
    "cols_10k = set(df_10k.columns)\n",
    "common_cols = cols_5k & cols_10k\n",
    "unique_to_5k = cols_5k - cols_10k\n",
    "unique_to_10k = cols_10k - cols_5k\n",
    "\n",
    "print(f\"\\nSchema Analysis\")\n",
    "print(f\"✓ Common columns ({len(common_cols)}): {sorted(common_cols)}\")\n",
    "print(f\"✓ Unique to 5K dataset ({len(unique_to_5k)}): {sorted(unique_to_5k)}\")\n",
    "print(f\"✓ Unique to 10K dataset ({len(unique_to_10k)}): {sorted(unique_to_10k)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e09ba7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Missing Values Analysis (Dataset 1 - 5K) ---\n",
      "      Column  Missing Count  Missing %\n",
      "    homepage           3091      64.36\n",
      "     tagline            844      17.57\n",
      "    overview              3       0.06\n",
      "     runtime              2       0.04\n",
      "release_date              1       0.02\n",
      "\n",
      "--- Missing Values Analysis (Dataset 2 - 10K) ---\n",
      "  Column  Missing Count  Missing %\n",
      "overview              6       0.06\n"
     ]
    }
   ],
   "source": [
    "# Missing values analysis\n",
    "print(\"\\n--- Missing Values Analysis (Dataset 1 - 5K) ---\")\n",
    "missing_5k = df_5k.isnull().sum()\n",
    "missing_5k_pct = (missing_5k / len(df_5k) * 100).round(2)\n",
    "missing_5k_df = pd.DataFrame({\n",
    "    'Column': missing_5k.index,\n",
    "    'Missing Count': missing_5k.values,\n",
    "    'Missing %': missing_5k_pct.values\n",
    "}).sort_values('Missing Count', ascending=False)\n",
    "print(missing_5k_df[missing_5k_df['Missing Count'] > 0].to_string(index=False))\n",
    "\n",
    "print(\"\\n--- Missing Values Analysis (Dataset 2 - 10K) ---\")\n",
    "missing_10k = df_10k.isnull().sum()\n",
    "missing_10k_pct = (missing_10k / len(df_10k) * 100).round(2)\n",
    "missing_10k_df = pd.DataFrame({\n",
    "    'Column': missing_10k.index,\n",
    "    'Missing Count': missing_10k.values,\n",
    "    'Missing %': missing_10k_pct.values\n",
    "}).sort_values('Missing Count', ascending=False)\n",
    "print(missing_10k_df[missing_10k_df['Missing Count'] > 0].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2dd540",
   "metadata": {},
   "source": [
    "### Step 2.1: Schema Summary\n",
    "\n",
    "**Key Insights:**\n",
    "- Both datasets share core columns: `id`, `title`, `original_language`, `original_title`, `overview`, `popularity`, `release_date`, `vote_average`, `vote_count`\n",
    "- Dataset 1 (5K) has financial data: `budget`, `revenue` (critical for our prediction task)\n",
    "- Dataset 1 (5K) has rich metadata: `genres`, `keywords`, `production_companies`, `production_countries`, `spoken_languages`\n",
    "- Dataset 2 (10K) appears to be a superset that may contain the same movies as 5K\n",
    "- Missing values are significant in both datasets - need careful imputation strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34e7fa61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Duplicate Detection by TMDB ID ---\n",
      "Duplicates in 5K dataset (by id): 0 records\n",
      "Duplicates in 10K dataset (by id): 0 records\n",
      "\n",
      "✓ After removing intra-dataset duplicates:\n",
      "  Dataset 1: 4803 → 4803 rows\n",
      "  Dataset 2: 10000 → 10000 rows\n",
      "\n",
      "--- Inter-Dataset Overlap ---\n",
      "✓ Movies in both datasets: 3297\n",
      "✓ Movies only in 5K: 1506\n",
      "✓ Movies only in 10K: 6703\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Duplicate Detection by TMDB ID ---\")\n",
    "\n",
    "# Check duplicates within each dataset\n",
    "dup_5k_id = df_5k[df_5k.duplicated(subset=['id'], keep=False)]\n",
    "dup_10k_id = df_10k[df_10k.duplicated(subset=['id'], keep=False)]\n",
    "\n",
    "print(f\"Duplicates in 5K dataset (by id): {len(dup_5k_id)} records\")\n",
    "print(f\"Duplicates in 10K dataset (by id): {len(dup_10k_id)} records\")\n",
    "\n",
    "# Remove intra-dataset duplicates (keep first occurrence)\n",
    "df_5k_dedup = df_5k.drop_duplicates(subset=['id'], keep='first')\n",
    "df_10k_dedup = df_10k.drop_duplicates(subset=['id'], keep='first')\n",
    "\n",
    "print(f\"\\n✓ After removing intra-dataset duplicates:\")\n",
    "print(f\"  Dataset 1: {df_5k.shape[0]} → {df_5k_dedup.shape[0]} rows\")\n",
    "print(f\"  Dataset 2: {df_10k.shape[0]} → {df_10k_dedup.shape[0]} rows\")\n",
    "\n",
    "# Check for inter-dataset overlap (movies present in both datasets)\n",
    "ids_5k = set(df_5k_dedup['id'].values)\n",
    "ids_10k = set(df_10k_dedup['id'].values)\n",
    "overlap_ids = ids_5k & ids_10k\n",
    "\n",
    "print(f\"\\n--- Inter-Dataset Overlap ---\")\n",
    "print(f\"✓ Movies in both datasets: {len(overlap_ids)}\")\n",
    "print(f\"✓ Movies only in 5K: {len(ids_5k - ids_10k)}\")\n",
    "print(f\"✓ Movies only in 10K: {len(ids_10k - ids_5k)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6580d7c",
   "metadata": {},
   "source": [
    "### Step 3.1: Deduplication Strategy\n",
    "\n",
    "**Approach:**\n",
    "1. **Primary Key**: Use TMDB `id` field (globally unique identifier)\n",
    "2. **Intra-dataset deduplication**: Remove duplicates within each dataset\n",
    "3. **Inter-dataset merge**: Keep all unique movies from both datasets\n",
    "4. **Preference**: For overlapping movies, prefer Dataset 1 (5K) as it has richer financial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88b3062c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Standardizing Dataset 1 (5K) ---\n",
      "✓ Converted 'release_date' to datetime\n",
      "✓ Standardized numeric columns: ['budget', 'revenue', 'runtime', 'popularity', 'vote_average', 'vote_count']\n",
      "\n",
      "--- Standardizing Dataset 2 (10K) ---\n",
      "✓ Converted 'release_date' to datetime\n",
      "✓ Standardized numeric columns: ['popularity', 'vote_average', 'vote_count']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_443/3367850433.py:20: Pandas4Warning: For backward compatibility, 'str' dtypes are included by select_dtypes when 'object' dtype is specified. This behavior is deprecated and will be removed in a future version. Explicitly pass 'str' to `include` to select them, or to `exclude` to remove them and silence this warning.\n",
      "See https://pandas.pydata.org/docs/user_guide/migration-3-strings.html#string-migration-select-dtypes for details on how to write code that works with pandas 2 and 3.\n",
      "  string_cols = df.select_dtypes(include=['object']).columns\n",
      "/tmp/ipykernel_443/3367850433.py:20: Pandas4Warning: For backward compatibility, 'str' dtypes are included by select_dtypes when 'object' dtype is specified. This behavior is deprecated and will be removed in a future version. Explicitly pass 'str' to `include` to select them, or to `exclude` to remove them and silence this warning.\n",
      "See https://pandas.pydata.org/docs/user_guide/migration-3-strings.html#string-migration-select-dtypes for details on how to write code that works with pandas 2 and 3.\n",
      "  string_cols = df.select_dtypes(include=['object']).columns\n"
     ]
    }
   ],
   "source": [
    "def standardize_dataframe(df, dataset_name):\n",
    "   \n",
    "    df = df.copy()\n",
    "    \n",
    "    print(f\"\\n--- Standardizing {dataset_name} ---\")\n",
    "    \n",
    "    # 1. Standardize release_date to datetime\n",
    "    if 'release_date' in df.columns:\n",
    "        df['release_date'] = pd.to_datetime(df['release_date'], errors='coerce')\n",
    "        print(f\"✓ Converted 'release_date' to datetime\")\n",
    "    \n",
    "    # 2. Standardize numeric columns\n",
    "    numeric_cols = ['budget', 'revenue', 'runtime', 'popularity', 'vote_average', 'vote_count']\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    print(f\"✓ Standardized numeric columns: {[c for c in numeric_cols if c in df.columns]}\")\n",
    "    \n",
    "    # 3. Ensure string columns are lowercase (for consistency in categorical data)\n",
    "    string_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in string_cols:\n",
    "        if col not in ['genres', 'keywords', 'production_companies', 'production_countries', 'spoken_languages']:\n",
    "            # Only lowercase simple text fields, not JSON-like data\n",
    "            df[col] = df[col].fillna('').astype(str)\n",
    "    \n",
    "    # 4. Drop rows with missing critical identifiers\n",
    "    if 'id' in df.columns:\n",
    "        initial_rows = len(df)\n",
    "        df = df.dropna(subset=['id'])\n",
    "        dropped = initial_rows - len(df)\n",
    "        if dropped > 0:\n",
    "            print(f\"✓ Dropped {dropped} rows with missing 'id'\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Standardize both datasets\n",
    "df_5k_std = standardize_dataframe(df_5k_dedup, \"Dataset 1 (5K)\")\n",
    "df_10k_std = standardize_dataframe(df_10k_dedup, \"Dataset 2 (10K)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71e5a72",
   "metadata": {},
   "source": [
    "### Step 4.1: Data Standardization Details\n",
    "\n",
    "**Transformations Applied:**\n",
    "1. **Date fields**: Convert to `datetime64` format for time-series analysis\n",
    "2. **Numeric fields**: Convert to float/int with proper handling of missing values\n",
    "3. **String fields**: Ensure consistent formatting (handle nulls, whitespace)\n",
    "4. **Critical fields**: Remove rows with missing TMDB IDs (primary key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6377e1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Merge Strategy ---\n",
      "✓ Base dataset (5K): 4803 movies\n",
      "✓ Additional movies from 10K: 6703 movies\n",
      "✓ Expected consolidated dataset: 11506 movies\n",
      "\n",
      "✓ Total unique columns in merged dataset: 20\n",
      "\n",
      "--- Consolidation Results ---\n",
      "✓ Final consolidated dataset shape: 11506 rows × 20 columns\n",
      "✓ Columns in consolidated dataset: ['budget', 'genres', 'homepage', 'id', 'keywords', 'original_language', 'original_title', 'overview', 'popularity', 'production_companies', 'production_countries', 'release_date', 'revenue', 'runtime', 'spoken_languages', 'status', 'tagline', 'title', 'vote_average', 'vote_count']\n"
     ]
    }
   ],
   "source": [
    "# Get IDs from standardized datasets\n",
    "ids_5k_std = set(df_5k_std['id'].values)\n",
    "ids_10k_std = set(df_10k_std['id'].values)\n",
    "\n",
    "# Find movies only in 10K\n",
    "unique_to_10k_ids = ids_10k_std - ids_5k_std\n",
    "df_10k_unique = df_10k_std[df_10k_std['id'].isin(unique_to_10k_ids)]\n",
    "\n",
    "print(f\"\\n--- Merge Strategy ---\")\n",
    "print(f\"✓ Base dataset (5K): {len(df_5k_std)} movies\")\n",
    "print(f\"✓ Additional movies from 10K: {len(df_10k_unique)} movies\")\n",
    "print(f\"✓ Expected consolidated dataset: {len(df_5k_std) + len(df_10k_unique)} movies\")\n",
    "\n",
    "all_cols = set(df_5k_std.columns) | set(df_10k_unique.columns)\n",
    "print(f\"\\n✓ Total unique columns in merged dataset: {len(all_cols)}\")\n",
    "\n",
    "# Perform the merge: reindex df_10k_unique to match df_5k_std columns, then add any extra columns\n",
    "cols_to_add = [col for col in df_10k_unique.columns if col not in df_5k_std.columns]\n",
    "df_10k_unique_reindexed = df_10k_unique.reindex(columns=df_5k_std.columns, fill_value=np.nan)\n",
    "\n",
    "df_consolidated = pd.concat([\n",
    "    df_5k_std,\n",
    "    df_10k_unique_reindexed\n",
    "], ignore_index=True, sort=False)\n",
    "\n",
    "print(f\"\\n--- Consolidation Results ---\")\n",
    "print(f\"✓ Final consolidated dataset shape: {df_consolidated.shape[0]} rows × {df_consolidated.shape[1]} columns\")\n",
    "print(f\"✓ Columns in consolidated dataset: {sorted(df_consolidated.columns.tolist())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "405f5409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Data Quality Check ---\n",
      "✓ Unique movies (by id): 11506\n",
      "✓ Duplicate IDs remaining: 0\n",
      "✓ ✓ ✓ NO DUPLICATES - Consolidation successful!\n",
      "\n",
      "--- Feature Availability in Consolidated Dataset ---\n",
      "  Total movies: 11506\n",
      "  With budget data: 4803 (41.7%)\n",
      "  With revenue data: 4803 (41.7%)\n",
      "  With both budget & revenue: 4803 (41.7%)\n"
     ]
    }
   ],
   "source": [
    "# Verify the consolidation\n",
    "print(\"\\n--- Data Quality Check ---\")\n",
    "print(f\"✓ Unique movies (by id): {df_consolidated['id'].nunique()}\")\n",
    "print(f\"✓ Duplicate IDs remaining: {len(df_consolidated[df_consolidated.duplicated(subset=['id'], keep=False)])}\")\n",
    "\n",
    "# Verify no data loss\n",
    "if df_consolidated['id'].nunique() == len(df_consolidated):\n",
    "    print(\"✓ ✓ ✓ NO DUPLICATES - Consolidation successful!\")\n",
    "else:\n",
    "    print(\"⚠ WARNING: Duplicates detected after consolidation!\")\n",
    "    \n",
    "# Data availability summary\n",
    "print(\"\\n--- Feature Availability in Consolidated Dataset ---\")\n",
    "data_available = {\n",
    "    'Total movies': len(df_consolidated),\n",
    "    'With budget data': df_consolidated['budget'].notna().sum() if 'budget' in df_consolidated.columns else 0,\n",
    "    'With revenue data': df_consolidated['revenue'].notna().sum() if 'revenue' in df_consolidated.columns else 0,\n",
    "    'With both budget & revenue': (df_consolidated['budget'].notna() & df_consolidated['revenue'].notna()).sum() if 'budget' in df_consolidated.columns and 'revenue' in df_consolidated.columns else 0,\n",
    "}\n",
    "for key, value in data_available.items():\n",
    "    pct = (value / data_available['Total movies'] * 100) if key != 'Total movies' else 0\n",
    "    if key != 'Total movies':\n",
    "        print(f\"  {key}: {value} ({pct:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eff6e6",
   "metadata": {},
   "source": [
    "### Step 5.1: Consolidated Dataset Summary\n",
    "\n",
    "The consolidation process:\n",
    "1. **Preserves** all movies from the 5K dataset (prioritized for data richness)\n",
    "2. **Adds** unique movies from the 10K dataset that weren't in 5K\n",
    "3. **Maintains** all available columns from both datasets\n",
    "4. **Ensures** no data duplication (verified by TMDB ID uniqueness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "876b0a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Consolidated dataset saved to: /home/asif/AI-Movie-Hit-Predictor/project_components/data/03_consolidated_movies.csv\n",
      "\n",
      "--- Consolidated Dataset Statistics ---\n",
      "Shape: (11506, 20)\n",
      "\n",
      "Data Types:\n",
      "budget                         float64\n",
      "genres                          object\n",
      "homepage                        object\n",
      "id                               int64\n",
      "keywords                        object\n",
      "original_language                  str\n",
      "original_title                     str\n",
      "overview                           str\n",
      "popularity                     float64\n",
      "production_companies            object\n",
      "production_countries            object\n",
      "release_date            datetime64[us]\n",
      "revenue                        float64\n",
      "runtime                        float64\n",
      "spoken_languages                object\n",
      "status                          object\n",
      "tagline                         object\n",
      "title                              str\n",
      "vote_average                   float64\n",
      "vote_count                       int64\n",
      "dtype: object\n",
      "\n",
      "\n",
      "First 5 rows (sample):\n",
      "       id                                     title       budget  \\\n",
      "0   19995                                    Avatar  237000000.0   \n",
      "1     285  Pirates of the Caribbean: At World's End  300000000.0   \n",
      "2  206647                                   Spectre  245000000.0   \n",
      "3   49026                     The Dark Knight Rises  250000000.0   \n",
      "4   49529                               John Carter  260000000.0   \n",
      "\n",
      "        revenue release_date  popularity  \n",
      "0  2.787965e+09   2009-12-10  150.437577  \n",
      "1  9.610000e+08   2007-05-19  139.082615  \n",
      "2  8.806746e+08   2015-10-26  107.376788  \n",
      "3  1.084939e+09   2012-07-16  112.312950  \n",
      "4  2.841391e+08   2012-03-07   43.926995  \n",
      "\n",
      "\n",
      "Descriptive Statistics (numeric columns):\n",
      "             budget            id    popularity                release_date  \\\n",
      "count  4.803000e+03  1.150600e+04  11506.000000                       11505   \n",
      "mean   2.904504e+07  1.577033e+05     22.706410  2003-09-17 10:48:35.671447   \n",
      "min    0.000000e+00  3.000000e+00      0.000000         1895-06-10 00:00:00   \n",
      "25%    7.900000e+05  1.055925e+04      8.387042         1997-09-11 00:00:00   \n",
      "50%    1.500000e+07  3.239250e+04     12.820388         2008-07-18 00:00:00   \n",
      "75%    4.000000e+07  2.932660e+05     21.820250         2015-07-31 00:00:00   \n",
      "max    3.800000e+08  1.024530e+06   9137.939000         2022-11-25 00:00:00   \n",
      "std    4.072239e+07  2.117455e+05    106.339770                         NaN   \n",
      "\n",
      "            revenue      runtime  vote_average    vote_count  \n",
      "count  4.803000e+03  4801.000000  11506.000000  11506.000000  \n",
      "mean   8.226064e+07   106.875859      6.451869    867.949070  \n",
      "min    0.000000e+00     0.000000      0.000000      0.000000  \n",
      "25%    0.000000e+00    94.000000      5.900000    221.000000  \n",
      "50%    1.917000e+07   103.000000      6.500000    382.000000  \n",
      "75%    9.291719e+07   118.000000      7.100000    838.000000  \n",
      "max    2.787965e+09   338.000000     10.000000  25809.000000  \n",
      "std    1.628571e+08    22.611935      0.991382   1596.821925  \n"
     ]
    }
   ],
   "source": [
    "output_path = DATA_DIR / \"03_consolidated_movies.csv\"\n",
    "df_consolidated.to_csv(output_path, index=False)\n",
    "print(f\"\\n✓ Consolidated dataset saved to: {output_path}\")\n",
    "\n",
    "# Generate summary statistics\n",
    "print(\"\\n--- Consolidated Dataset Statistics ---\")\n",
    "print(f\"Shape: {df_consolidated.shape}\")\n",
    "print(f\"\\nData Types:\")\n",
    "print(df_consolidated.dtypes)\n",
    "\n",
    "print(f\"\\n\\nFirst 5 rows (sample):\")\n",
    "print(df_consolidated[['id', 'title', 'budget', 'revenue', 'release_date', 'popularity']].head())\n",
    "\n",
    "print(f\"\\n\\nDescriptive Statistics (numeric columns):\")\n",
    "print(df_consolidated.describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
